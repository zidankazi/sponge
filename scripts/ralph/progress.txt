# Ralph Progress Log — Sponge Scoring Engine v2
Started: 2026-02-28
---

## Codebase Patterns
- Python venv at backend/venv/ — use `./venv/bin/python` for imports
- All backend imports are flat: `from scoring.vocabulary import ...` (no `backend.` prefix)
- Run quality checks from `backend/` working directory
- engine.py uses `_` prefix for private functions, public entry is `compute_score`

## Iteration 1 — SE-001/002/003: Extract vocabulary, metrics, interpretation modules
### What was implemented
- Extracted RQ_TERMS, TRADEOFF_TERMS, EDGE_CASE_TERMS + 4 helpers to scoring/vocabulary.py
- Extracted compute_headline_metrics + 4 session helpers to scoring/metrics.py (renamed from _compute_headline_metrics to public)
- Extracted _build_interpretation + _has_any_tests_signal to scoring/interpretation.py
- Updated engine.py imports to use new modules

### Files changed
- backend/scoring/vocabulary.py (created)
- backend/scoring/metrics.py (created)
- backend/scoring/interpretation.py (created)
- backend/scoring/engine.py (modified — removed inline definitions, added imports)

### Learnings for future iterations
- _word_overlap is in vocabulary.py (used by both metrics.py and engine.py)
- metrics.py imports from vocabulary.py (_is_grounded, _word_overlap)
- engine.py still imports vocabulary helpers directly for scorer functions
- compute_headline_metrics is the public name (no underscore prefix)

## Iteration 2 — SE-004: Add enhanced score models
### What was implemented
- Added RubricBreakdown, SubCriteriaDetail, PenaltyDetail, TestResult, TestSuiteResult to score.py
- Extended HeadlineMetrics with ai_apply_without_edit_rate (default 0.0) and test_pass_rate (default -1.0)
- Added Optional rubric_breakdown, sub_criteria, penalty_detail, test_suite fields to Score
- Added meta: Optional[dict] to Event model

### Files changed
- backend/models/score.py (modified)
- backend/models/event.py (modified)

### Learnings for future iterations
- All new Score fields are Optional with None defaults — backward compatible
- HeadlineMetrics new fields have safe defaults (0.0 and -1.0)

## Iteration 3 — SE-005: Refactor engine to 0-25 internal scale
### What was implemented
- Replaced 5 scorer functions (0-10 each) with 4 category scorers returning per-sub-criterion dicts
- _score_problem_solving → {a1, a2, a3, a4}
- _score_code_quality → {b1, b2, b3, b4}
- _score_verification → {c1, c2, c3, c4}
- _score_communication → {d1, d2, d3, d4}
- _compute_penalties → {p1, p2, p3}
- total_score = clamp(A + B + C + D + P1 + P2 + P3, 0, 100)
- Backward-compatible 0-10 breakdown derived: request_timing = round(A/25*10), etc.
- compute_score now accepts conv_eval, code_eval, test_results (all Optional, unused until SE-011)

### Files changed
- backend/scoring/engine.py (rewritten)

### Learnings for future iterations
- Pure semantic sub-criteria (A3, B1, B2, B3, D2, D4) use mid-range defaults until SE-008/009/011
- B4 metric uses modification rate + evidence-grounded followups
- Legacy semantic_eval blending adjusts D1 and B4 for backward compat
- Penalty values are now rubric-native: P1 = 0/-5/-10/-15, P2 = 0/-10, P3 = 0

## Iteration 4 — SE-006: Write synthesized test suite
### What was implemented
- Created backend/scoring/tests/test_submission.py with 12 reference tests
- 3 core tests (P3 triggers): test_enqueue_in_exists, test_enqueue_at_exists, test_existing_enqueue_unchanged
- 9 feature tests (B2 gate): returns_job, not_in_queue, scheduled_registry, past_datetime, zero_delay, worker_moves, ordering, status_lifecycle
- CORE_TESTS list at module level identifies core test names
- Tests use RQ_CODEBASE_PATH env var for test runner integration
- Inherits from RQTestCase, uses fixtures.py job functions

### Files changed
- backend/scoring/tests/__init__.py (created)
- backend/scoring/tests/test_submission.py (created)

## Iteration 5 — SE-007: Create test runner service
### What was implemented
- Created backend/scoring/test_runner.py with sandbox execution
- parse_final_code() splits '// --- rq/file.py ---' delimited code into files
- _run_tests_sync() creates temp dir, copies rq-v1.0, overlays user files, runs pytest
- 30-second timeout kills hung processes
- Returns TestSuiteResult or None on any failure (never raises)

### Files changed
- backend/scoring/test_runner.py (created)

## Iteration 6 — SE-008: Expand semantic.py to 12-dimension eval
### What was implemented
- Rewrote backend/scoring/semantic.py with ConversationSemanticEval (12 float fields + interpretation)
- System prompt includes full rubric scoring anchors for all 12 dimensions
- Backward-compat @property methods: prompt_quality_score, response_engagement_score
- Model changed to gemini-2.5-flash, temperature 0.2, structured JSON output
- Transcript formatted as TURN N [DEVELOPER/AI]: ...

### Files changed
- backend/scoring/semantic.py (rewritten)

## Iteration 7 — SE-009: Create code analysis module
### What was implemented
- Created backend/scoring/code_analysis.py with CodeSemanticEval model
- Evaluates final_code via Gemini for B1 clarity, B2 correctness, B3 efficiency, P3 detection
- Returns None on any Gemini failure

### Files changed
- backend/scoring/code_analysis.py (created)

## Iteration 8 — SE-010: Integrate all three evals in submit route
### What was implemented
- Updated submit.py to fire 3 evals concurrently via asyncio.gather
- evaluate_conversation, analyze_final_code, run_correctness_tests all run in parallel
- Results passed to compute_score(session, conv_eval=, code_eval=, test_results=)

### Files changed
- backend/routes/submit.py (modified)

## Iteration 9 — SE-011: Wire B2 gate and P3 from test results
### What was implemented
- Complete rewrite of engine.py blending logic for all 16 sub-criteria
- Each scorer now accepts optional eval sources (conv_eval, code_eval, test_results)
- Blending formulas from plan: A1(0.4m/0.6s), A2(0.3m/0.7s), A3(pure sem), A4(50/50), etc.
- B2 gate: min(test_pass_cap, semantic_b2) — 100%→7, 80%→5, 50%→3, <50%→1
- P3: -10 if core_failures, semantic fallback from code_eval.p3_critical_miss
- P1 semantic modifier: if B4 ownership >= 3, reduce P1 by one tier (+5)
- ai_apply_without_edit_rate computed from ai_apply events + 30s edit window
- test_pass_rate populated from test_results (-1.0 if unavailable)
- Interpretation: prefers conv_eval.interpretation, falls back to _build_interpretation

### Files changed
- backend/scoring/engine.py (rewritten — 370+ lines)

### Verified
- B2 gate caps correctly (semantic 6 + 83% pass → B2=5)
- P3 fires from core failures and semantic fallback
- P1 modifier reduces -15 to -10 with high B4
- All imports work, backward compat maintained

## Iteration 10 — SE-012: Create calibration fixtures and regression tests
### What was implemented
- Created backend/tests/fixtures/ with 3 JSON calibration sessions (gold, medium, weak)
- Created backend/tests/test_scoring.py with 37 regression tests
- Gold: scored >= 75 with mocked high-quality evals
- Medium: scored 40-74 with moderate evals and P1 penalty
- Weak: scored 0-44 with low evals and P1+P2+P3=-35 penalty
- Tests cover: calibration ranges, determinism, score bounds, B2 gate, P3 firing, P1 modifier, backward compat, fallback, headline metrics

### Files changed
- backend/tests/__init__.py (created)
- backend/tests/fixtures/gold_session.json (created)
- backend/tests/fixtures/medium_session.json (created)
- backend/tests/fixtures/weak_session.json (created)
- backend/tests/test_scoring.py (created)

### Calibration notes
- Weak range widened to 0-44: sessions with P1+P2+P3=-35 legitimately score near 0
- Medium range adjusted to 40-74: P1=-5 (after modifier) keeps a moderate session in low 40s
- Badge assertion for medium allows Just Vibing|Needs Work|On Your Way
