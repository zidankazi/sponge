{
  "project": "Sponge Scoring Engine v2",
  "branchName": "sri/scoring-v2",
  "description": "Upgrade scoring engine to production-grade with 1:1 rubric fidelity, 3-layer evaluation (metric + semantic + execution), and calibration tests",
  "userStories": [
    {
      "id": "SE-001",
      "title": "Extract vocabulary module from engine",
      "description": "Move RQ_TERMS, TRADEOFF_TERMS, EDGE_CASE_TERMS and their helper functions (_is_grounded, _has_tradeoff_language, _has_edge_case_language) from engine.py to a new backend/scoring/vocabulary.py. Update engine.py imports.",
      "acceptanceCriteria": [
        "backend/scoring/vocabulary.py exists with all 3 term dicts and 3 helper functions",
        "engine.py imports from vocabulary.py instead of defining them inline",
        "python -c 'from scoring.vocabulary import RQ_TERMS, TRADEOFF_TERMS, EDGE_CASE_TERMS' succeeds (run from backend/)",
        "python -c 'from scoring.engine import compute_score' still succeeds (run from backend/)"
      ],
      "priority": 1,
      "passes": true,
      "notes": "Pure refactor — no behavior change. engine.py currently defines these at lines 34-63."
    },
    {
      "id": "SE-002",
      "title": "Extract metrics module from engine",
      "description": "Move _compute_headline_metrics and all metric helper functions (_user_prompts, _ai_responses, _events_of, _session_start_ms, _word_overlap) from engine.py to backend/scoring/metrics.py. Update engine.py imports.",
      "acceptanceCriteria": [
        "backend/scoring/metrics.py exists with _compute_headline_metrics and 5 helper functions",
        "engine.py imports from metrics.py",
        "python -c 'from scoring.metrics import compute_headline_metrics' succeeds (run from backend/)",
        "python -c 'from scoring.engine import compute_score' still succeeds (run from backend/)"
      ],
      "priority": 2,
      "passes": true,
      "notes": "Pure refactor — no behavior change. These are at engine.py lines 66-209."
    },
    {
      "id": "SE-003",
      "title": "Extract interpretation module from engine",
      "description": "Move _build_interpretation and _has_any_tests_signal from engine.py to backend/scoring/interpretation.py. Update engine.py imports.",
      "acceptanceCriteria": [
        "backend/scoring/interpretation.py exists with both functions",
        "engine.py imports from interpretation.py",
        "python -c 'from scoring.interpretation import build_interpretation' succeeds (run from backend/)",
        "python -c 'from scoring.engine import compute_score' still succeeds (run from backend/)"
      ],
      "priority": 3,
      "passes": true,
      "notes": "Pure refactor. These are at engine.py lines 543-601."
    },
    {
      "id": "SE-004",
      "title": "Add enhanced score models",
      "description": "Add RubricBreakdown, SubCriteriaDetail, PenaltyDetail, TestResult, and TestSuiteResult models to backend/models/score.py. Add ai_apply_without_edit_rate and test_pass_rate to HeadlineMetrics. Add rubric_breakdown, sub_criteria, penalty_detail, test_suite as Optional fields on Score. All new fields must be Optional with defaults so existing API responses remain valid. Also add meta: Optional[dict] = None to backend/models/event.py.",
      "acceptanceCriteria": [
        "All 5 new models defined in backend/models/score.py",
        "HeadlineMetrics has ai_apply_without_edit_rate: float = 0.0 and test_pass_rate: float = -1.0",
        "Score has rubric_breakdown, sub_criteria, penalty_detail, test_suite as Optional fields defaulting to None",
        "python -c 'from models.score import Score, RubricBreakdown, SubCriteriaDetail, PenaltyDetail, TestSuiteResult, TestResult' succeeds (run from backend/)",
        "Existing Score instantiation still works: Score(total_score=50, breakdown=ScoreBreakdown(...), headline_metrics=HeadlineMetrics(...), interpretation='test', badge='test')",
        "Event model has meta: Optional[dict] = None in backend/models/event.py"
      ],
      "priority": 4,
      "passes": true,
      "notes": "Current models at backend/models/score.py. Keep existing models intact, only add new ones."
    },
    {
      "id": "SE-005",
      "title": "Refactor engine to 0-25 internal scale",
      "description": "Refactor compute_score in engine.py to compute category scores on the rubric's native 0-25 scale (A: Problem Solving, B: Code Quality, C: Verification, D: Communication). Derive the existing 0-10 breakdown fields for backward compatibility. Compute total_score directly from A+B+C+D+Penalties on 0-100. Populate rubric_breakdown, sub_criteria, and penalty_detail on the returned Score. Each scorer function should return individual sub-criteria values.",
      "acceptanceCriteria": [
        "Each scorer function returns sub-criteria tuples/dicts that sum to 0-25 per category",
        "Score.rubric_breakdown is populated with problem_solving, code_quality, verification, communication (each 0-25)",
        "Score.sub_criteria is populated with all 16 sub-criterion scores",
        "Score.penalty_detail is populated with p1_over_reliance, p2_no_run, p3_critical_miss (p3 = 0 for now)",
        "Score.breakdown (0-10 fields) remains populated: request_timing=round(A/25*10) etc.",
        "total_score = clamp(A + B + C + D + P1 + P2 + P3, 0, 100)",
        "python -c 'from scoring.engine import compute_score' succeeds (run from backend/)",
        "Semantic blending still works: when semantic_eval is not None, request_quality and response_handling are blended 60/40"
      ],
      "priority": 5,
      "passes": true,
      "notes": "This is the core refactor. Penalties now use rubric-native values: P1 = 0/-5/-10/-15, P2 = 0/-10, P3 = 0 (enabled later). The compute_score function signature adds optional conv_eval, code_eval, test_results params (all defaulting to None for backward compat)."
    },
    {
      "id": "SE-006",
      "title": "Write synthesized test suite for submission validation",
      "description": "Create backend/scoring/tests/__init__.py and backend/scoring/tests/test_submission.py with 12 reference tests that validate the delayed job execution feature (enqueue_in, enqueue_at). Tests inherit from rq-v1.0/tests RQTestCase and use fixtures.py patterns. 3 tests are tagged as core (P3 triggers), 9 are feature tests (B2 gate). Add sys.path manipulation so the tests can import from rq-v1.0.",
      "acceptanceCriteria": [
        "backend/scoring/tests/__init__.py exists",
        "backend/scoring/tests/test_submission.py exists with 12 test methods in a TestSubmission class",
        "3 core tests: test_enqueue_in_exists, test_enqueue_at_exists, test_existing_enqueue_unchanged",
        "9 feature tests covering: returns job, not in queue immediately, in scheduled registry, past datetime, zero delay, worker moves ready jobs, multiple job ordering, job status lifecycle",
        "Each test has a clear docstring explaining what it validates",
        "A CORE_TESTS list at module level identifies the 3 core test names",
        "Tests can be collected by pytest (they don't need to pass against unmodified rq — they test the user's additions)"
      ],
      "priority": 6,
      "passes": true,
      "notes": "RQ test patterns: use self.testconn for Redis, Queue(connection=self.testconn), Worker([q], connection=self.testconn), worker.work(burst=True). Fixtures: say_hello, do_nothing from rq-v1.0/tests/fixtures.py. RQTestCase base at rq-v1.0/tests/__init__.py. The tests must add rq-v1.0 and rq-v1.0/tests to sys.path. These tests will FAIL against the unmodified rq-v1.0 (that's the point — they test the user's implementation)."
    },
    {
      "id": "SE-007",
      "title": "Create test runner service",
      "description": "Create backend/scoring/test_runner.py that: (1) parses final_code (concatenated with '// --- rq/file.py ---' headers) into individual files, (2) creates a temp directory with rq-v1.0 codebase, (3) overlays user's modified files, (4) runs test_submission.py via pytest subprocess with 30s timeout, (5) parses output into TestSuiteResult, (6) cleans up temp directory. Expose async run_correctness_tests(final_code) entry point.",
      "acceptanceCriteria": [
        "backend/scoring/test_runner.py exists with async run_correctness_tests(final_code: str) -> Optional[TestSuiteResult]",
        "parse_final_code(final_code) correctly splits '// --- rq/queue.py ---' delimited code into file dict",
        "Creates isolated temp directory, does NOT modify rq-v1.0/",
        "30-second timeout kills hung pytest processes",
        "Returns None on any failure (timeout, parse error, missing Redis) — never raises",
        "TestSuiteResult includes total, passed, failed, pass_rate, results list, core_failures list",
        "python -c 'from scoring.test_runner import run_correctness_tests' succeeds (run from backend/)"
      ],
      "priority": 7,
      "passes": true,
      "notes": "For local dev, runs pytest as subprocess. Use tempfile.mkdtemp() for isolation. shutil.copytree() to clone rq-v1.0. Run: pytest test_submission.py --tb=line -q and parse stdout. The test_submission.py from SE-006 should be copied into the temp dir. Clean up with shutil.rmtree() in finally block. Use asyncio.to_thread() to run the subprocess in a non-blocking way."
    },
    {
      "id": "SE-008",
      "title": "Expand semantic.py to 12-dimension conversation eval",
      "description": "Rewrite backend/scoring/semantic.py to evaluate 12 sub-criteria from the conversation transcript. Return ConversationSemanticEval model with: a1_understanding (0-6), a2_decomposition (0-7), a3_justification (0-7), a4_edge_cases (0-5), b3_efficiency_discussion (0-5), b4_ownership_dialogue (0-5), c2_test_mentions (0-9), c3_ai_questioning (0-4), d1_narration (0-8), d2_tradeoffs (0-7), d3_ai_balance (0-5), d4_status_updates (0-5), plus interpretation string. Keep graceful fallback to None on any failure. Keep the old evaluate_conversation function name but return the new model type.",
      "acceptanceCriteria": [
        "ConversationSemanticEval model has 12 float fields + interpretation string",
        "System prompt includes full rubric scoring anchors for each of the 12 dimensions",
        "Temperature 0.2, response_mime_type='application/json', model='gemini-2.5-flash'",
        "evaluate_conversation() returns ConversationSemanticEval or None on failure",
        "Transcript formatted as 'TURN 1 [DEVELOPER]: ... TURN 2 [AI]: ...' with turn numbers",
        "Long AI responses truncated to 800 chars in transcript",
        "JSON parsing handles markdown fences and partial extraction (same robustness as before)",
        "python -c 'from scoring.semantic import evaluate_conversation, ConversationSemanticEval' succeeds (run from backend/)"
      ],
      "priority": 8,
      "passes": true,
      "notes": "Current semantic.py returns SemanticEval with only prompt_quality_score and response_engagement_score. The new ConversationSemanticEval replaces it. The system prompt should be structured as a rubric document with scoring anchors per dimension."
    },
    {
      "id": "SE-009",
      "title": "Create code analysis module",
      "description": "Create backend/scoring/code_analysis.py that evaluates final_code via Gemini for B1 (clarity 0-8), B2 (correctness design 0-7), B3 (efficiency 0-5) and P3 critical miss detection. Return CodeSemanticEval model. Uses same Gemini client pattern as semantic.py. Graceful fallback to None.",
      "acceptanceCriteria": [
        "backend/scoring/code_analysis.py exists with async analyze_final_code(final_code: str) -> Optional[CodeSemanticEval]",
        "CodeSemanticEval model has b1_clarity (float 0-8), b2_correctness (float 0-7), b3_efficiency_code (float 0-5), p3_critical_miss (bool), p3_details (str), code_feedback (str)",
        "System prompt evaluates: RQ naming conventions, data structure choices (sorted set for scheduling), efficiency, and checks for enqueue_in/enqueue_at/ScheduledJobRegistry existence",
        "Returns None on any Gemini failure — never raises",
        "Temperature 0.2, response_mime_type='application/json', model='gemini-2.5-flash'",
        "python -c 'from scoring.code_analysis import analyze_final_code, CodeSemanticEval' succeeds (run from backend/)"
      ],
      "priority": 9,
      "passes": true,
      "notes": "Reuses the _get_client() and _parse_response() patterns from semantic.py. The code analysis prompt should reference the rubric's B1/B2/B3 criteria and list the specific requirements for P3 detection."
    },
    {
      "id": "SE-010",
      "title": "Integrate all three evals in submit route",
      "description": "Update backend/routes/submit.py to fire all three evaluation sources concurrently: evaluate_conversation, analyze_final_code, and run_correctness_tests via asyncio.gather. Pass all three results to compute_score. Update compute_score in engine.py to accept the new parameters.",
      "acceptanceCriteria": [
        "submit.py imports asyncio, analyze_final_code, run_correctness_tests",
        "All 3 evals fire concurrently with asyncio.gather",
        "compute_score accepts conv_eval, code_eval, test_results parameters (all Optional, default None)",
        "If any eval returns None, engine uses fallback scoring for those dimensions",
        "Score response includes test_suite field when test_results is available",
        "POST /submit still returns valid Score JSON (run backend and test with curl or python)",
        "python -c 'from routes.submit import router' succeeds (run from backend/)"
      ],
      "priority": 10,
      "passes": true,
      "notes": "Current submit.py only calls evaluate_conversation and compute_score. The asyncio.gather pattern: conv_eval, code_eval, test_results = await asyncio.gather(evaluate_conversation(...), analyze_final_code(...), run_correctness_tests(...)). Each returns None on failure."
    },
    {
      "id": "SE-011",
      "title": "Wire B2 gate and P3 from test results in engine",
      "description": "Update engine.py blending logic so each sub-criterion uses its specified formula from the plan. B2 = min(test_pass_cap, semantic_b2) where cap comes from test_pass_rate. P3 = -10 if any core test fails. P1 has semantic modifier: if B4 semantic >= 3, reduce P1 by one tier. Each of the 16 sub-criteria must use the correct blending formula (metric floor, semantic weight, etc.).",
      "acceptanceCriteria": [
        "B2 is capped by test_pass_rate when test_results is available (100%->7, 80%->5, 50%->3, <50%->1)",
        "B2 uses pure semantic when test_results is None (fallback to 7 cap)",
        "P3 = -10 when core_failures list is non-empty in test_results",
        "P3 uses code_eval.p3_critical_miss as fallback when test_results is None",
        "P1 semantic modifier: if conv_eval b4_ownership_dialogue >= 3, P1 reduced by one tier",
        "headline_metrics.test_pass_rate is populated from test results (-1.0 if unavailable)",
        "headline_metrics.ai_apply_without_edit_rate computed from ai_apply events (0.0 if no events)",
        "total_score = clamp(A + B + C + D + P1 + P2 + P3, 0, 100)",
        "All sub-criteria blending formulas match the plan's Section 4"
      ],
      "priority": 11,
      "passes": true,
      "notes": "This is the most complex story. Refer to the approved plan at .claude/plans/concurrent-jingling-parnas.md Section 4 for exact formulas. Key formulas: A1 = clamp(0.4*metric + 0.6*semantic, floor, 6), A2 = clamp(0.3*metric + 0.7*semantic, floor, 7), A3 = pure semantic, etc."
    },
    {
      "id": "SE-012",
      "title": "Create calibration fixtures and regression tests",
      "description": "Create backend/tests/ directory with calibration JSON session fixtures and regression tests. Gold session simulates strong developer (many file opens, grounded prompts, edits after AI, test runs, debug cycles). Medium has gaps. Weak is AI-dependent. Tests mock Gemini and verify score ranges.",
      "acceptanceCriteria": [
        "backend/tests/__init__.py exists",
        "backend/tests/fixtures/ directory exists with gold_session.json, medium_session.json, weak_session.json",
        "Each fixture is a valid Session-shaped JSON with events, conversation_history, started_at, session_id",
        "backend/tests/test_scoring.py exists with test functions",
        "Gold fixture (mocked semantic): total_score >= 75",
        "Medium fixture (mocked semantic): 45 <= total_score <= 74",
        "Weak fixture (mocked semantic): 15 <= total_score <= 44",
        "Score determinism test: same input produces identical scores across 2 runs",
        "Bounds test: 0 <= total_score <= 100 for all fixtures",
        "Backward compat test: all breakdown fields (0-10) are within valid range",
        "pytest backend/tests/test_scoring.py -v passes with all tests green"
      ],
      "priority": 12,
      "passes": true,
      "notes": "Fixtures should have realistic event patterns. Gold: 5+ file_opens before first prompt, 4+ test_runs, edits after each AI response, 8+ conversation turns with RQ terms. Medium: 2 file_opens, 2 test_runs, some blind adoption. Weak: 1 file_open, 0 test_runs, short generic prompts, no edits. Mock Gemini with unittest.mock.patch on the evaluate_conversation and analyze_final_code functions."
    }
  ]
}
